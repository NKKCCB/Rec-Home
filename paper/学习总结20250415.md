信息抽取实践-PP UIE（LLM大模型用于信息抽取）

**1.1理论学习：**

1.1.1前置知识：

span corruption训练

Span Corruption 是T5（Text-To-Text Transfer Transformer） 预训练任务之一，其将完整的句子根据随机的span进行掩码。

如：原句：“Thank you for inviting me to your party last week”Span Corruption之后得到输入： “Thank you [X] me to your party [Y] week”；目标：“[X] for inviting [Y] last [Z]”。其中 [X] 等一系列辅助编码称为 sentinels。

信息抽取的两种范式？
有两种：
（1）NLU式（抽取式）：输入→编码器（如Bert，CNN，RNN）→解码器（如softmax，crf）→抽取的信息（如实体、关系、事件）
（2）NLG式（生成式）：输入+Prompt→解码器（如各种生成式大模型）→生成的信息（如实体、关系、事件）
“前者以bert为代表，我们通常选取一个预训练完成的bert模型作为编码器，然后根据目标任务在bert输出后手动添加解码层，如在实体抽取任务中添加一个crf层。在训练时可以选择冻结BERT全部参数或者微调较高的几层。
后者通常以如今的大模型为代表，如GPT4、Qwen等，端到端的信息抽取，输入文本和Prompt，直接输出期望的结果和格式。
两者对比，NLU的特点是需要针对任务单独训练解码器才能工作，这意味着需要准备去一定量的标注数据，属于监督学习。NLG的特点是零样本或者少样本学习，拿来即用，虽然其性能不如NLU（如BERT是双向学习更能理解语义信息），但在很多场景下，少样本学习够用了，不需要在训练泛化性好（省事），目前NLG的方式逐渐成为主流。但如果对抽取的质量有严格的要求（比如医疗、金融、安全领域），或者容易获取标注数据，那么NLU是更更适合的方式。”



**1.2百度通用抽取模型UIE--Unified Structure Generation for Universal Information Extraction fromACL2022**

![image-20250417135606973](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417135606973.png)

1.2.1动机

​		提出通用信息抽取统一框架

1.2.2做法

​		1.统一训练形式

​		作者本文提出了两个东西：

​	（1）SEL (结构抽取语言)  编码不同的信息抽取结构。这样所有的信息抽取任务就有了一样的输入数据结构。

SEL把所有信息抽取任务都抽象出生成SPOTNAME、ASSONAME、INFOSPAN三种。

- SPOTNAME：表示原文片段所属的实体类型

- ASSONAME：表示不同片段之间的关联关系

- INFOSPAN：表示对应spotting和associating所在原文中的信息

​	（2）SSI (结构化模式指导器) 一种基于Schema的提示机制。当输入句子时，在句子前面拼接上对应的Prompt做提示，如：[spot]人物[spot] 组织机构 [spot] 时间 [text] 1997年，史蒂夫兴奋地成为苹果公司的首席执行官。[spot]人物[spot] 组织机构 [spot] 时间 [text]都是prompt

​			2.训练

​			预训练包括三个部分

​	（1）使用 SSI+ text + SEL 训练并行对(token序列x，结构化记录y) 。 训练得到文本到结构的转化能力。

使模型能用:[spot] person [asso] work for [text]Steve became CEO of Apple in 1997., 生成:((person: Steve(work for: Apple)))。

​	（2）只使用SEL作为训练数据。结构化记录 y（None，None，y），使用前一部分来生成后一部分，并且只训练UIE的decoder部分，学习SEL语法能力。

使模型能够 ：输入((person: Steve(work for。生成:: Apple)))。

​	（3）使用text和SEL 。构造无结构的原始文本数据：（None，x'(破坏过的源文本），x''(破坏的目标spans)） 训练得到 语义编码能力（span corruption）



**1.3实践：**

**1.3.1百度PP-UIE通用信息抽取大模型**

​		使用百度paddleNLP框架新开源发布的信息抽取模型进行信息抽取，本地调试跑通，以下是demo

![QQ20250416-200741](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/QQ20250416-200741.png)

​		对法律文书pdf文件进行信息抽取

![image-20250417184340264](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417184340264.png)

抽取效果不佳，待调试

**1.3.2源码阅读：**

以推理过程为例

```python
ie = Taskflow(
    task="information_extraction",
    model="paddlenlp/PP-UIE-0.5B",
    schema=schema,
    batch_size=3, # 批处理大小，处理多个输入样本时，可以加速处理速度，但会增加内存消耗。默认值为1
    precision="float16",
)
```

调用方法UIELLMTask._multi_stage_predict

![image-20250417192004478](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417192004478.png)

对每个实体进行挨个抽取

![image-20250417192206330](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417192206330.png)

```python
result_list = self._single_stage_predict(examples)
```

注入prompt

![image-20250417192508425](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417192508425.png)

对prompt注入对话模板

```
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
你是一个阅读理解专家，请提取所给句子与问题，提取实体。请注意，如果存在实体，则一定在原句中逐字出现，请输出对应实体的原文，不要进行额外修改；如果无法提取，请输出“无相应实体”。
**句子开始**
被告人朱某某于2018年4月25日至4月27日期间，先后在常州市钟楼区**镇**东大门**路**号门口、**村委**村**号**楼**房间**路**物流港**幢*号**楼一房间等地盗窃作案3次，窃得被害人吴某某、周某某、王某某的电动车、手机等物品。
**句子结束**
**问题开始**
时间
**问题结束**
**回答开始**
<|im_end|>
<|im_start|>assistant
```

将对话转换为该模型对应的word映射的词向量

```python
tokenized_output = self._tokenizer(
    input_text,
    return_tensors="pd",
    return_position_ids=True,
    padding_side="left",
    padding=True,
    max_new_tokens=self._max_seq_length,
    truncation=True,
    truncation_side="left",
    add_special_tokens=self._tokenizer.chat_template is None,
)
```

![image-20250417193225079](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417193225079.png)

进行真正编码阶段，这里可以看到编码器参数全部固定的

```python
@paddle.no_grad()
def generate(
```

这里加载参数加载了几种id配置：

![image-20250417194759894](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417194759894.png)

eos_token_id ：表示段落的结束。在生成长篇文本时，可以将 `eop_token` 插入到段落的结尾，以指示模型生成新的段落。

bos_token_id：表示句子的开头。在使用生成模型生成文本时，可以将 `bos_token` 插入到输入文本的开头，以指示模型开始生成新的句子。

pad_token_id：pad_token主要是用来补齐一个batch中各个样本的长度的

decoder_start_token_id：
“为什么我们需要一个decoder_start_token_id呢？在Transformer的解码器中，生成过程是通过不断预测下一个标记来完成的。在一般的文本生成任务中，我们只需要在输入序列之后追加一个特殊的起始标记，然后使用模型从该标记开始生成即可。但在某些任务中，特别是带有固定输入和输出长度的任务，decoder_start_token_id的作用就显得更加重要。

举个例子，假设我们想要使用BART模型生成一段固定长度的摘要。首先，我们将输入文本进行编码，并将编码结果作为encoder的输出。然后，在解码过程中，我们需要一个decoder_start_token_id来告诉模型从哪里开始生成摘要。这个起始标记可以是一个特殊的标记，如[CLS]（表示一个句子的开头）或[SOS]（表示开始生成）。通过提供decoder_start_token_id，BART模型就知道了从哪里开始进行生成，并且可以控制生成的长度以符合所需的摘要长度。”

forced_bos_token_id：解码器在生成 `decoder_start_token_id` 对应token之后指定生成的token id，mBART这种多语言模型会用到，因为这个值一般用来区分target语种

forced_eos_token_id：达到最大长度 `max_length` 时，强制作为最后生成的token id

预测下一token直至到eos位置，这里调用基础LLM模型的生成接口

```python
# pre-process distribution
next_token_logits = self.adjust_logits_during_generation(next_token_logits)
probs = logits_processors(input_ids, next_token_logits)
# greedy
next_tokens = paddle.argmax(probs, axis=-1).unsqueeze(-1)
next_scores = paddle.index_sample(probs, next_tokens)
```

并解码

```python
out_list = []
for x in results:
    res = self._tokenizer.decode(x.numpy().tolist(), skip_special_tokens=True)
    res = res.strip("\n")
    end_idx = res.find("\n**回答结束**")
    if end_idx != -1:
        res = res[:end_idx]
    out_list.append([{"text": res}])
```

遍历待抽取的实体列表，输出最终结果