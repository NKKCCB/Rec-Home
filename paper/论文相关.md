# 方法借鉴

### 1.多轮收集(也是自己检查自己)

为了在效率和质量之间取得平衡，使用多轮“收集”来确保所有实体都被检测到。 这种方法允许使用较大的文本块大小，而不会降低提取质量或引入噪音。
第一步：要求LLM评估是否所有实体都已提取，强制做出是/否决定。
如果LLM回答遗漏了实体，继续提示“上次提取中遗漏了许多实体”，鼓励LLM收集这些遗漏的实体。

### 2.自己生成自己检查

命题生成

```
def generate_propositions(chunk):
    """
    从文本块中生成原子化、自包含的命题。

    Args:
        chunk (Dict): 包含内容和元数据的文本块

    Returns:
        List[str]: 生成的命题列表
    """
    # 系统提示，指示AI如何生成命题
    system_prompt = """请将以下文本分解为简单的自包含命题。确保每个命题符合以下标准：

    1. 表达单一事实：每个命题应陈述一个具体事实或主张
    2. 独立可理解：命题应自成体系，无需额外上下文即可理解
    3. 使用全称而非代词：避免使用代词或模糊指代，使用完整的实体名称
    4. 包含相关日期/限定条件：如适用应包含必要日期、时间和限定条件以保持准确性
    5. 保持单一主谓关系：聚焦单个主体及其对应动作或属性，避免连接词和多从句结构

    请仅输出命题列表，不要包含任何额外文本或解释，格式为命题列表。
    """
```

命题质量核查

```
def evaluate_proposition(proposition, original_text):
    """
    根据准确性、清晰性、完整性以及简洁性评估命题的质量。

    Args:
        proposition (str): 要评估的命题
        original_text (str): 用于比较的原文

    Returns:
        Dict: 每个评估维度的分数
    """
    # 系统提示，指示AI如何评估命题
    system_prompt = """你是一位评估从文本中提取命题质量的专家。请根据以下标准对给定命题进行评分（1-10分）：

    - 准确性（Accuracy）：命题反映原文信息的准确程度
    - 清晰性（Clarity）：不依赖额外上下文的情况下，命题是否易于理解
    - 完整性（Completeness）：命题是否包含必要的细节（如日期、限定词等）
    - 简洁性（Conciseness）：命题是否在保留关键信息前提下，表述精简程度

    响应必须为有效的JSON格式，并包含每个标准的数值评分：
    {"accuracy": X, "clarity": X, "completeness": X, "conciseness": X}
    """
```

### 3.融合检索：结合向量与关键词搜索

- 从 PDF 文件中提取文本
- 使用 jieba 分词器对文本进行分词，并创建向量存储
- 使用 BM25 算法对查询进行关键词匹配
- 使用向量搜索对查询进行语义匹配
- 将两种方法的结果进行加权组合，并重新排序
- 返回最终的搜索结果

### 4.用于RAG的分级索引

实现一种用于RAG系统的分级索引方法(Hierarchical Indices)。这种技术通过使用两级搜索方法来提高检索效果：首先通过摘要识别相关的文档部分，然后从这些部分中检索具体细节。

------

传统的RAG方法将所有文本块一视同仁，这可能导致：

- 当文本块过小时，上下文信息丢失
- 当文档集合较大时，检索结果无关
- 在整个语料库中搜索效率低下

------

分级检索解决了这些问题，具体方式如下：

- 为较大的文档部分创建简洁的摘要
- 首先搜索这些摘要以确定相关部分
- 然后仅从这些部分中检索详细信息
- 在保留具体细节的同时保持上下文信息

------

实现步骤：

- 从 PDF 中提取页面
- 为每一页创建摘要，将摘要文本和元数据添加到摘要列表中
- 为每一页创建详细块，将页面的文本切分为块
- 为以上两个创建嵌入，并行其存入向量存储中
- 使用查询分层检索相关块：先检索相关的摘要，收集来自相关摘要的页面，然后过滤掉不是相关页面的块，从这些相关页面检索详细块
- 根据检索到的块生成回答