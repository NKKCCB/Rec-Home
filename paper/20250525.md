## 1.python新特性

<img src="C:\Users\ZhuanZ\AppData\Roaming\Typora\typora-user-images\image-20250525154000240.png" alt="image-20250525154000240" style="zoom: 67%;" />

等同于：

![image-20250525153949019](C:\Users\ZhuanZ\AppData\Roaming\Typora\typora-user-images\image-20250525153949019.png)

## 2.deepseed

​	DeepSpeed是一个由微软开发的开源深度学习优化库，旨在**提高大规模模型训练的效率和可扩展性**。它通过多种技术手段来加速训练，包括模型并行化、梯度累积、动态精度缩放、本地模式混合精度等



## 3.In-context learning

​	In-context Learning 语境学习是一种学习范式，是现代自然语言处理领域中一种重要的学习方法，尤其在使用大规模，尤其在使用大规模预训练模型时，它允许模型在给定的上下文中进行学习和推理，而无需真正更新模型参数。

**In-context Learning的工作原理**

提示词和示例

**提示词**：ICL常通过提示词来引导模型的生成过程，提示词通常包括任务描述，问题陈述或请求模型执行的操作。‍‍‍‍‍‍‍‍‍
示例：在少样本学习(Few-Shot Learning)中，提示词可能包括一些示例输入和输出，帮助模型理解如何处理类似的任务。‍‍
上下文提供

**任务描述**：在ICL中，任务描述用于告诉模型要完成的任务，例如：生成一个关于人工智能的总结。‍‍‍‍‍
示例输入输出：提供几个示例输入和输出对，可以帮助模型理解特定任务的模式或要求，例如：给出一些翻译示例来帮助模型进行语言翻译。‍‍
推理和生成‍‍‍

**推理**：模型根据提供的上下文进行推理，生成与上下文相关的响应或输出。‍‍‍‍‍‍
生成：在ICL中，生成的文本基于模型对上下文的理解，以及预训练中学到的知识‍‍‍

## 4.[Bi-encoder vs Cross encoder? - 青铜时代的猪 - 博客园](https://www.cnblogs.com/wanger-sjtu/p/18227082)

or这个网址 ： https://wanger-sjtu.github.io/encoder-cross-bi/

## 5.Retrieval-based Language Models 





## 6.[DPO: Direct Preference Optimization 直接偏好优化](https://www.cnblogs.com/lemonzhang/p/17910358.html)

## 7.Scaling Laws 

Scaling Laws的核心是：**模型性能主要取决于参数量（N）、数据量（D）和计算量（C）**。在其他条件相近时，这三大要素的变化会引起模型性能以**可预期的方式**改变。

**Scaling Laws 在实践中具有重要指导意义：**

- ***预测和规划模型训练：\*** 通过在小规模模型上实验并拟合Scaling Law，研究者可以**预测更大模型的性能**，从而决定训练多大的模型、用多少数据，以高效利用有限的计算预算。例如，在构建“前沿”大模型时，通常会先确定可用的算力预算（如总共可执行多少FLOPs），再利用Scaling Law计算出**理想的参数量和数据集大小**。然后据此设计模型和准备数据，进行预训练直至耗尽既定算力为止。这种方法已成为业界训练数百亿甚至上千亿参数模型的基础流程。
- ***优化资源分配：\*** Scaling Law揭示了**参数规模、数据规模与算力之间的最佳配比**。OpenAI的研究发现，在给定计算预算下，与其训练一个小模型用尽海量数据，不如训练一个**更大的模型**但**适度早停**更为高效。更大的模型对数据的**样本效率**更高——达到同等性能所需的训练步骤和数据更少。因此，在资源受限的情况下，通常应优先增加模型参数，而非一味增加训练轮数或数据量。这一指导原则直接影响了GPT-3等模型的训练策略：OpenAI据此大胆地构建了千亿级参数模型，并在训练未完全收敛时就停止，结果证明大型模型依然取得了优秀性能。
- ***评估模型扩展性：\*** 通过Scaling Law，可以评估模型性能是否达到了当前规模的**饱和点**。如果模型仍遵循预期的幂律提升，则意味着增加规模仍有益；反之，如果性能提升出现停滞或偏离幂律曲线，则提示可能需要调整策略（如改进模型架构或算法）而不仅仅是“堆料”扩展。

## 8.[KL散度理解以及使用pytorch计算KL散度 - 知乎](https://zhuanlan.zhihu.com/p/339613080)

## 9：held-in held-out

.held-in evaluation内部评估，使用同一数据集进行训练和评估，通常采用交叉验证技术将数据集划分为训练集和验证集。模型会在训练集上进行训练，并使用验证集上的数据进行评估和调优。可能导致过度拟合。
held-out evaluation：外部评估，使用独立的、未参与训练的数据集进行评估的。测试泛化能力。

## 10.F1 这会别忘了额

![image-20250529163321555](C:\Users\ZhuanZ\AppData\Roaming\Typora\typora-user-images\image-20250529163321555.png)

## 11.解决使用git时遇到Failed to connect 

Failed to connect to github.com port 443 after 21090 ms: Couldn‘t connect to server[解决使用git时遇到Failed to connect to github.com port 443 after 21090 ms: Couldn‘t connect to server_git couldn't connect to server-CSDN博客](https://blog.csdn.net/qq_40296909/article/details/134285451)

## 12.实体链接任务

**实体链接（entity linking）** 任务是指识别出文本中的**提及（mention）**、确定mention的含义并消除其可能存在的歧义，然后建立该mention到 **知识库（KB）** 中 **实体（entity）** 的链接，从而将非[结构化数据](https://so.csdn.net/so/search?q=结构化数据&spm=1001.2101.3001.7020)连接到结构化数据的过程。

## 13.SASREC 代码

模型的输入是通过采样一个batch_size的序列，在代码中，有这一句代码进行实现。

```python
sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen, n_workers=3)
```

采样方法

```python
def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):
    def sample():
        # 随机从用户集中采样一个用户，用户用id来表示
        user = np.random.randint(1, usernum + 1)
        # 如果训练集中，该样本的交互历史小于1，那就重新采样
        while len(user_train[user]) <= 1: user = np.random.randint(1, usernum + 1) 
        seq = np.zeros([maxlen], dtype=np.int32) # 序列
        pos = np.zeros([maxlen], dtype=np.int32) # 用于存储交互的正样本
        neg = np.zeros([maxlen], dtype=np.int32) # 用于存储负样本
        nxt = user_train[user][-1] # 最后一个交互的物品
        idx = maxlen - 1 
        ts = set(user_train[user]) 
        for i in reversed(user_train[user][:-1]): # reversed是逆序搜索，这里的i指的是交互的物品
            seq[idx] = i
            pos[idx] = nxt
            # 为什么要设定不等于0？是为了保证当序列长度没到达maxlen时，正样本序列会补充为0，那么构成的负样本序列也应该是0
            if nxt != 0: neg[idx] = random_neq(1, itemnum + 1, ts)  
            nxt = i
            idx -= 1
            if idx == -1: break
        return (user, seq, pos, neg)
    return sample()
```

SASREC 模型输入

```python
pos_logits, neg_logits = model(u, seq, pos, neg)
```

forward中特征编码

```
log_feats = self.log2feats(log_seqs)
```

具体流程

```python
def log2feats(self, log_seqs):
        # 取出物品id对应的embedding
        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.dev))
         # 为什么要做这个操作? 其实就是乘以一个embedding维度的根号值，感觉有点类似于attention的计算除以根号d那样
        seqs *= self.item_emb.embedding_dim ** 0.5    
        # 构建出位置的矩阵
        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1]) #
        # 取出位置对应的embedding
        seqs += self.pos_emb(torch.LongTensor(positions).to(self.dev))
        seqs = self.emb_dropout(seqs)
        # 如果序列太短，前面等于0，那么对应的序列embedding也要等于0
        timeline_mask = torch.BoolTensor(log_seqs == 0).to(self.dev)
        seqs *= ~timeline_mask.unsqueeze(-1) # broadcast in last dim
        # 这个是为了实现论文中提到的前面物品在预测时不能用到后面物品的信息，用mask来实现
        tl = seqs.shape[1] # time dim len for enforce causality
        attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))
        # 送入模型，进行预测
        for i in range(len(self.attention_layers)):
            seqs = torch.transpose(seqs, 0, 1)
            Q = self.attention_layernorms[i](seqs)
            mha_outputs, _ = self.attention_layers[i](Q, seqs, seqs, 
                                            attn_mask=attention_mask)
                                            # key_padding_mask=timeline_mask
                                            # need_weights=False) this arg do not work?
            seqs = Q + mha_outputs
            seqs = torch.transpose(seqs, 0, 1)

            seqs = self.forward_layernorms[i](seqs)
            seqs = self.forward_layers[i](seqs)
            seqs *=  ~timeline_mask.unsqueeze(-1)
        log_feats = self.last_layernorm(seqs) # (U, T, C) -> (U, -1, C)
        return log_feats
```

作者在文中没有提及对比学习，但是在代码中有使用，会计算正负样本的损失

```python
loss = bce_criterion(pos_logits[indices], pos_labels[indices])
loss += bce_criterion(neg_logits[indices], neg_labels[indices])
```

## 14.模型蒸馏及代码实现

先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近emsemble的结果

传统训练：当没有 Teacher 网络时候，仅仅将 data 经过 Student 网络，在softmax之后，输出概率分布值 q，将 q 与 label p 求 cross_entropy loss 就是称为 Hard loss，因为这个p是真实值的one-hot向量，q和p越接近越好。

知识蒸馏：当有 Teacher 的帮助下的时候，loss来自 Student 和 Teacher 网络。且Teacher 输出的 q' 要经过带温度的Softmax之后（让它更加平滑，思想类似于label smooth）得到 q'' 再与 q 求loss，总loss = Teacher q'' 和 Student q 的 loss + Student q 和 label p 的 loss。

![image-20250128123453391](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250128123453391.png)

其核心蒸馏损失的计算如下:

```python
def distillation_loss(y,labels,teacher_scores,temp,alpha):
	soft_loss = nn.KLDivLoss()(F.log_softmax(y/temp, dim=1), F.softmax(teacher_scores/temp,dim=1))
	hard_loss = F.cross_entropy(y,labels)
    return soft_loss *(temp*temp*2.0*alpha) + hard_loss *(1. - alpha)

```

## 15.胶囊网络

2.1原理

胶囊网络的设计初衷主要来自于解决两个问题：**局部敏感性**和**层次结构解析能力的不足**。胶囊网络的设计初衷主要来自于解决两个问题：**局部敏感性**和**层次结构解析能力的不足**。

局部敏感性：传统的 CNN 在图像识别任务中表现优秀，但它们对于输入的微小变化非常敏感。例如，稍微旋转或平移一个图像可能导致 CNN 的输出发生显著变化。层次结构解析能力的不足：CNN 主要关注局部特征，并可能忽略这些特征如何在更高层次上组织成有用的结构。这就导致了它们在理解复杂空间层次关系方面的不足。

![image-20250131122456469](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250131122456469.png)

胶囊网络输出是一个高维向量。这个输出向量的模长通常用于表示某种特定特征是存在概率，而向量的方向则用于编码该特征的更多属性：如位置、方向、大小等。

动态路由：能够在不同胶囊之间传递信息，从而使得网络能够更好地理解对象的内部组成结构和**相对**空间关系。

胶囊网络中的 “胶囊” 是一组神经元，其输出为向量。向量的长度可表示实体的某种属性（如物体的存在概率），向量方向则代表实体的其他特性（如物体的姿态等） 。动态路由根据输入信息调整不同胶囊之间的连接强度。较低层胶囊的输出会根据与较高层各个胶囊的 匹配度，动态分配权重并传递信息，以确定哪些特征更重要。

2.2代码实现

动态路由 通过迭代优化路由权重，增强胶囊间的一致性

```python
def dynamic_routing(logits, num_iterations=3):
    b = torch.zeros_like(logits) #路由权重初始化为零
    for i in range(num_iterations):  
        c = F.softmax(b, dim=2)  # 通过Softmax函数计算路由权重	
        s = (c * logits).sum(dim=1)
        v = squash(s)  # 使用squash函数将s压缩为非线性向量，归一化
        if i < num_iterations -1:  # #计算预测向量与输出向量的相似度，用于更新路由权重，多次迭代
            agreement = (logits * v.unsqueeze(1)).sum(dim=3)
            b = b + agreement
    return v
```

网络forward

```python
 def forward(self, x, y=None):
        x = F.relu(self.conv1(x))
        x = self.primary_capsules(x) # 卷积层输出n个xx维向量，然后展平
        batch_size = x.size(0)
        x = x.view(batch_size, self.num_primary_capsules, self.primary_capsule_dim)
        x = squash(x)
        digit_caps = self.digit_capsules(x)  # 数字胶囊层，输出n个xx维的胶囊,n对应兴趣数量
        probs = digit_caps.norm(dim=2)
        if y is None: 
            _, max_length_indices = probs.max(dim=1)
            y = torch.eye(10).to(x.device).index_select(dim=0, index=max_length_indices)
        recon = self.decoder((digit_caps * y.unsqueeze(2)).view(batch_size, -1)) # 解码器，用于重建输入
        recon = recon.view(-1, 1, 28, 28)
        return probs, recon

```

## 16.MOE原理以及手撕学习

MoE架构的主要特点是在模型中引入了专家网络层，通过**路由机制(Routing function)**选择激活哪些专家，以允许不同的专家模型对输入进行独立处理，并通过**加权组合它们的输出**来生成最终的预测结果。

“*MoE提出的前提是如果有一个包括了多个领域知识的复杂问题，我们该使用什么样的方法来解决呢？最简单的办法就是把各个领域的专家集合到一起来攻克这个任务，当然我们事先要把[不同的](https://so.csdn.net/so/search?q=不同的&spm=1001.2101.3001.7020)任务先分离出来，这样才便于分发给不同领域的专家，让他们来帮忙处理，最后再汇总结论。*”这也是大模型能实现大规模推理的原因之一，每次只激活部分参数。

### 16.1基础MOE demo

最基础的MOE实际上就是用 线形层等对输入计算出expert的概率分布，再用每个expert对输出进行计算，再按概率分布进行加权。

<img src="https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250216114016756.png" alt="image-20250216114016756" style="zoom:50%;" />

```python
class BasicMOE(nn.Module):
    def __init__(self, feature_in, feature_out, expert_number):
        super().__init__()
        self.experts = nn.ModuleList(
            [
                BasicExpert(feature_in, feature_out) for _ in range(expert_number)
            ]
        )
        # gate 就是选一个 expert 
        self.gate = nn.Linear(feature_in, expert_number)
        
    def forward(self, x):
        # x 的 shape 是 （batch, feature_in)
        expert_weight = self.gate(x)  # shape 是 (batch, expert_number)
        expert_out_list = [
            expert(x).unsqueeze(1) for expert in self.experts
        ]  # 里面每一个元素的 shape 是： (batch, ) ??
        # concat 起来 (batch, expert_number, feature_out)
        expert_output = torch.cat(expert_out_list, dim=1)
        # print(expert_output.size())
        expert_weight = expert_weight.unsqueeze(1) # (batch, 1, expert_number)
        # expert_weight * expert_out_list
        output = expert_weight @ expert_output  # (batch, 1, feature_out)
        
        return output.squeeze()
    
def test_basic_moe():
    x = torch.rand(2, 4)
    basic_moe = BasicMOE(4, 3, 2)
    out = basic_moe(x)
    print(out)
    
test_basic_moe()
```

### 16.2MOE plus版：[SparseMoE ]

<img src="https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250216114630287.png" alt="image-20250216114630287" style="zoom:50%;" />

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicExpert(nn.Module):
    # 一个 Expert 可以是一个最简单的， linear 层即可
    # 也可以是 MLP 层
    # 也可以是 更复杂的 MLP 层（active function 设置为 swiglu）
    def __init__(self, feature_in, feature_out):
        super().__init__()
        self.linear = nn.Linear(feature_in, feature_out)
    def forward(self, x):
        return self.linear(x)
class MOERouter(nn.Module):
    def __init__(self, hidden_dim, expert_number, top_k):
        super().__init__()
        self.gate = nn.Linear(hidden_dim, expert_number)
        self.expert_number = expert_number
        self.top_k = top_k
    def forward(self, hidden_states):
        # 计算路由logits
        router_logits = self.gate(hidden_states)  # shape is (b * s, expert_number)

        # 计算专家经过softmax之后的概率
        routing_probs = F.softmax(router_logits, dim=-1, dtype=torch.float)

        # 计算topk的专家的输出
        router_weights, selected_experts = torch.topk(
            routing_probs, self.top_k, dim=-1
        )  # shape都是 (b * s, top_k)

        # 专家权重归一化
        router_weights = router_weights / router_weights.sum(dim=-1, keepdim=True)
        router_weights = router_weights.to(hidden_states.dtype)

        # 生成专家掩码
        expert_mask = F.one_hot(
            selected_experts,
            num_classes=self.expert_number
        )  # shape是 (b * s, top_k, expert_number)
        expert_mask = expert_mask.permute(2, 1, 0)  # (expert_number, top_k, b * s)

        return router_logits, router_weights, selected_experts, expert_mask
class MOEConfig:
    def __init__(
            self,
            hidden_dim,
            expert_number,
            top_k,
            shared_experts_number=2,
    ):
        self.hidden_dim = hidden_dim
        self.expert_number = expert_number
        self.top_k = top_k
        self.shared_experts_number = shared_experts_number
class SparseMOE(nn.Module):
    # 稀疏 MOE 模型，这里每一个 token 都会过 topk 个专家，得到对应token 的 hidden_embeddings
    def __init__(self, config):
        super().__init__()
        self.hidden_dim = config.hidden_dim
        self.expert_number = config.expert_number
        self.top_k = config.top_k
        self.experts = nn.ModuleList(
            [
                BasicExpert(self.hidden_dim, self.hidden_dim) for _ in range(self.expert_number)
            ]
        )
        self.router = MOERouter(self.hidden_dim, self.expert_number, self.top_k)
    def forward(self, x):
        # x shape is (b, s, hidden_dim)
        batch_size, seq_len, hidden_dim = x.size()  # 2 4 16
        # 合并前两个维度，因为不是 Sample 维度了，而是 token 维度
        hidden_states = x.view(-1, hidden_dim)  # shape is(b * s, hidden_dim)

        router_logits, router_weights, selected_experts_indices, expert_mask = self.router(hidden_states)
        # 其中 selected_experts_indices shape 是 (b * s, top_k)
        # 其中 expert_mask shape 是 (expert_number, top_k, b * s)

        final_hidden_states = torch.zeros(
            (batch_size * seq_len, hidden_dim),
            dtype=hidden_states.dtype,
            device=hidden_states.device
        )
        for expert_idx in range(self.expert_number):
            expert_layer = self.experts[expert_idx]
            # expert_mask[expert_idx] shape 是 (top_k, b * s)
            idx, top_x = torch.where(expert_mask[expert_idx])
            # idx 和 top_x 都是一维 tensor
            # idx 的值是 0 或 1, 表示这个 token 是作为当前专家的 top1 还是 top2
            # top_x 的值是 token 在 batch*seq_len 中的位置索引
            # 例如对于 batch_size=2, seq_len=4 的输入:
            # top_x 的值范围是 0-7, 表示在展平后的 8 个 token 中的位置
            # idx 的值是 0/1, 表示这个 token 把当前专家作为其 top1/top2 专家

            # hidden_states 的 shape 是 (b * s, hidden_dim)
            # 需要取到 top_x 对应的 hidden_states
            current_state = hidden_states.unsqueeze(
                0
            )[:, top_x, :].reshape(-1, hidden_dim)  # （selected_token_number, hidden_dim）

            # router_weight 的 shape 是 (b * s, top_k)
            current_hidden_states = expert_layer(
                current_state
            ) * router_weights[top_x, idx].unsqueeze(-1)  # （selected_token_number, 1） 这里有广播
            # 把当前专家的输出加到 final_hidden_states 中
            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))
        # 把 final_hidden_states 还原到原来的 shape
        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)
        return final_hidden_states, router_logits  # shape 是 (b * s, expert_number)
def test_token_level_moe():
    x = torch.rand(2, 4, 16)
    config = MOEConfig(16, 2, 2)
    token_level_moe = SparseMOE(config)
    out = token_level_moe(x)
    print(out[0].shape, out[1].shape)

test_token_level_moe()
```

### 16.3 deepseek 版MOE

相较普通的多了一个 shared experts 模型，所有 token 都过这个 shared experts 模型，然后每个 token 会用计算的 Router 权重，来选择 topK 个专家，然后和共享的专家的输出一起加权求和。

<img src="https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250216192421393.png" alt="image-20250216192421393" style="zoom:50%;" />

deepseek的moe 中share experts相当于提供通用的特征提取，与“专精”的expert做加权

### 16.4  既然只选了top k个expert,如何反向传播？豆包的回答：按照前向时的权重算出对应expert的梯度

![image-20250216193555699](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250216193555699.png)

### 16.5 MMOE

**MMOE**(Multi-gate Mixture-of-Experts)是在MOE的基础上，使用了多个门控网络， ![k](https://www.zhihu.com/equation?tex=k&consumer=ZHI_MENG) 个任就对应 ![k](https://www.zhihu.com/equation?tex=k&consumer=ZHI_MENG) 个门控网络。所以在多任务时可以考虑这个MMOE。

“相对于 **MOE**的结构中所有任务共享一个门控网络，**MMOE**的结构优化为每个任务都单独使用一个门控网络。这样的改进可以针对不同任务得到不同的 Experts 权重，从而实现对 Experts 的选择性利用，不同任务对应的门控网络可以学习到不同的Experts 组合模式，因此模型更容易捕捉到子任务间的相关性和差异性。”

<img src="https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250216220659093.png" alt="image-20250216220659093" style="zoom:50%;" />

## 17.双塔网络 

 ![image-20250216220041771](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250216220041771.png)

## 18.排序算法的pairwise，pointwise

[排序算法的pairwise，pointwise，listwise](https://zhuanlan.zhihu.com/p/613354685)

## 19.推荐系统的主要四个阶段（召回、粗排、精排、重排）

[推荐系统的主要四个阶段（召回、粗排、精排、重排）](https://blog.csdn.net/qq_41750911/article/details/124573064)

## 20.[beam rearch](https://zhuanlan.zhihu.com/p/114669778) & n-gram & BERTscore

![image-20250623093022104](C:\Users\Dunker\AppData\Roaming\Typora\typora-user-images\image-20250623093022104.png)

n越小灵活度越高



BERTdscore （看起来不考虑词序）

![image-20250623094136851](C:\Users\Dunker\AppData\Roaming\Typora\typora-user-images\image-20250623094136851.png)

## 21.span corruption训练

Span Corruption 是T5（Text-To-Text Transfer Transformer） 预训练任务之一，其将完整的句子根据随机的span进行掩码。

如：原句：“Thank you for inviting me to your party last week”Span Corruption之后得到输入： “Thank you [X] me to your party [Y] week”；目标：“[X] for inviting [Y] last [Z]”。其中 [X] 等一系列辅助编码称为 sentinels。

## 22.信息抽取的两种范式？

有两种：
（1）NLU式（抽取式）：输入→编码器（如Bert，CNN，RNN）→解码器（如softmax，crf）→抽取的信息（如实体、关系、事件）
（2）NLG式（生成式）：输入+Prompt→解码器（如各种生成式大模型）→生成的信息（如实体、关系、事件）
“前者以bert为代表，我们通常选取一个预训练完成的bert模型作为编码器，然后根据目标任务在bert输出后手动添加解码层，如在实体抽取任务中添加一个crf层。在训练时可以选择冻结BERT全部参数或者微调较高的几层。
后者通常以如今的大模型为代表，如GPT4、Qwen等，端到端的信息抽取，输入文本和Prompt，直接输出期望的结果和格式。
两者对比，NLU的特点是需要针对任务单独训练解码器才能工作，这意味着需要准备去一定量的标注数据，属于监督学习。NLG的特点是零样本或者少样本学习，拿来即用，虽然其性能不如NLU（如BERT是双向学习更能理解语义信息），但在很多场景下，少样本学习够用了，不需要在训练泛化性好（省事），目前NLG的方式逐渐成为主流。但如果对抽取的质量有严格的要求（比如医疗、金融、安全领域），或者容易获取标注数据，那么NLU是更更适合的方式。”

## 23.LLM框架源码

以推理过程为例

```python
ie = Taskflow(
    task="information_extraction",
    model="paddlenlp/PP-UIE-0.5B",
    schema=schema,
    batch_size=3, # 批处理大小，处理多个输入样本时，可以加速处理速度，但会增加内存消耗。默认值为1
    precision="float16",
)
```

调用方法UIELLMTask._multi_stage_predict

![image-20250417192004478](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417192004478.png)

对每个实体进行挨个抽取

![image-20250417192206330](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417192206330.png)

```python
result_list = self._single_stage_predict(examples)
```

注入prompt

![image-20250417192508425](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417192508425.png)

对prompt注入对话模板

```
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
你是一个阅读理解专家，请提取所给句子与问题，提取实体。请注意，如果存在实体，则一定在原句中逐字出现，请输出对应实体的原文，不要进行额外修改；如果无法提取，请输出“无相应实体”。
**句子开始**
被告人朱某某于2018年4月25日至4月27日期间，先后在常州市钟楼区**镇**东大门**路**号门口、**村委**村**号**楼**房间**路**物流港**幢*号**楼一房间等地盗窃作案3次，窃得被害人吴某某、周某某、王某某的电动车、手机等物品。
**句子结束**
**问题开始**
时间
**问题结束**
**回答开始**
<|im_end|>
<|im_start|>assistant
```

将对话转换为该模型对应的word映射的词向量

```python
tokenized_output = self._tokenizer(
    input_text,
    return_tensors="pd",
    return_position_ids=True,
    padding_side="left",
    padding=True,
    max_new_tokens=self._max_seq_length,
    truncation=True,
    truncation_side="left",
    add_special_tokens=self._tokenizer.chat_template is None,
)
```

![image-20250417193225079](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417193225079.png)

进行真正编码阶段，这里可以看到编码器参数全部固定的

```python
@paddle.no_grad()
def generate(
```

这里加载参数加载了几种id配置：

![image-20250417194759894](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417194759894.png)

eos_token_id ：表示段落的结束。在生成长篇文本时，可以将 `eop_token` 插入到段落的结尾，以指示模型生成新的段落。

bos_token_id：表示句子的开头。在使用生成模型生成文本时，可以将 `bos_token` 插入到输入文本的开头，以指示模型开始生成新的句子。

pad_token_id：pad_token主要是用来补齐一个batch中各个样本的长度的

decoder_start_token_id：
“为什么我们需要一个decoder_start_token_id呢？在Transformer的解码器中，生成过程是通过不断预测下一个标记来完成的。在一般的文本生成任务中，我们只需要在输入序列之后追加一个特殊的起始标记，然后使用模型从该标记开始生成即可。但在某些任务中，特别是带有固定输入和输出长度的任务，decoder_start_token_id的作用就显得更加重要。

举个例子，假设我们想要使用BART模型生成一段固定长度的摘要。首先，我们将输入文本进行编码，并将编码结果作为encoder的输出。然后，在解码过程中，我们需要一个decoder_start_token_id来告诉模型从哪里开始生成摘要。这个起始标记可以是一个特殊的标记，如[CLS]（表示一个句子的开头）或[SOS]（表示开始生成）。通过提供decoder_start_token_id，BART模型就知道了从哪里开始进行生成，并且可以控制生成的长度以符合所需的摘要长度。”

forced_bos_token_id：解码器在生成 `decoder_start_token_id` 对应token之后指定生成的token id，mBART这种多语言模型会用到，因为这个值一般用来区分target语种

forced_eos_token_id：达到最大长度 `max_length` 时，强制作为最后生成的token id

预测下一token直至到eos位置，这里调用基础LLM模型的生成接口

```python
# pre-process distribution
next_token_logits = self.adjust_logits_during_generation(next_token_logits)
probs = logits_processors(input_ids, next_token_logits)
# greedy
next_tokens = paddle.argmax(probs, axis=-1).unsqueeze(-1)
next_scores = paddle.index_sample(probs, next_tokens)
```

并解码

```python
out_list = []
for x in results:
    res = self._tokenizer.decode(x.numpy().tolist(), skip_special_tokens=True)
    res = res.strip("\n")
    end_idx = res.find("\n**回答结束**")
    if end_idx != -1:
        res = res[:end_idx]
    out_list.append([{"text": res}])
```

遍历待抽取的实体列表，输出最终结果

# paper

## 1.MM-Rec: Multimodal News Recommendation--From SIGIR2022

![image-20250201173040031](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201173040031.png)

①动机。作者围绕多模态新闻推荐优势阐述了两点：a.用户点击会受图片与标题两方面影响 b.用户具有多兴趣，点击的新闻可能可能仅与某一兴趣有关，进行候选新闻与点击新闻的跨模态建模可以更好的分析相关性

②做法：用预训练的mask-rcnn提取roi，再对每个感兴趣区域用resnet提取特征，最终序列长度就是roi的数量。对序列建模，最终序列长度就是词长度。用预训练的 ViLBERT 模型捕捉标题和图像的内在相关性，通过co- Transformer 学习隐藏表示序列。再利用词注意力网络和图像注意力网络分别得到新闻标题和图像的最终表示。

为了解决点击新闻与候选新闻的匹配问题（去噪），作者提出了一个candidate-aware attention network 

![image-20250201172421112](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201172421112.png)

![image-20250201172437147](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201172437147.png)

将候选文本与点击图片、点击文本与候选图片计算权重（向量投影），并作为用户的兴趣表征

![image-20250201172837025](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201172837025.png)

最终根据用户表征与候选新闻的图片表征、文本表征计算权重（理解为向量投影，即相似度），计算出预测分数

## 2.When Multi-Level Meets Multi-Interest: A Multi-Grained Neural Model for Sequential Recommendation--From SIGIR2022

![image-20250201110944213](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201110944213.png)

①动机。现有的方法遵循两个方向改进序列推荐：a.对多兴趣进行聚类，并根据历史item的聚类结果进行推荐  b.利用图卷积对历史item之间的关系进行建模。本文则是期望提出一个结合多兴趣学习和多层级图卷积聚合二者优势的框架，能够解决不同颗粒度下的多兴趣问题。

②做法。作者首先将历史item转换为全连接的图，并使用capsnet进行更新，形成自适应图结构（不同商品之间的距离根据用户嵌入以及两个商品的嵌入联合计算得到。用户嵌入被用来实现用户感知的图构建，相同的两个商品对对于不同的用户可能具有不同的相关性值）。通过GCN提取多级商品表征（ L 层），每一层输出的商品表征代表不同层级的商品信息，对应着后续提取不同层级的用户兴趣

<img src="https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201174851118.png" alt="image-20250201174851118" style="zoom:50%;" />

在提取多级商品表征后，模型利用胶囊网络对用户每个层级的历史商品序列分别提取出 K 个兴趣向量。为了补充胶囊网络中的时序问题，作者采用 BiLSTM 对序列进行时序编码，模型利用带有时间特征的输出对动态路由的权重通过残差结构进行更新。最终，每个层级的序列商品表征都经过序列胶囊网络得到对应用户的 K 个兴趣向量。

在预测时对于L个层级的K个向量，作者使用池化获取最大概率的，作为最终预测的兴趣结果。

![image-20250201175217184](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201175217184.png)



## 3.Pre-training Graph Transformer with Multimodal Side Information for Recommendation --From ACM MM

![image-20250201180814847](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201180814847.png)

①动机。本文动机概括如下：利用side-information进行多模态自监督预训练，并且提出了名为MCNSampling的算法用于处理大规模的图数据

②做法。

PMGT包含四个主要组件：上下文邻居采样、点嵌入初始化、Transformer编码器、图重建

上下文邻居采样： 对于每个节点h，图中都存在一些相关的节点，这可能有助于丰富其表示。这些相关的节点被称为h的上下文邻居。为了在PMGT训练过程中有效地选择一批节点的上下文邻居，我们开发了一种名为MCN采样的采样算法。

点嵌入初始化： 在邻域采样后，将目标节点h与其有序的上下文邻域Sh连接起来，并加入了位置id embedding以记住位置信息

Transformer编码器：使用Transformer框架来建模一个节点与其上下文邻居之间的相关关系。

最终预训练流程如下：对item H进行上下文采样（每次都是从一个小批次数据中采样，H是个带有各种side information的节点），接着进行embedding为序列，对H的上下文进行随机掩码，例如掩盖20%的item，预测目标有两个：图结构、掩码节点特征。

<img src="https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201190515432.png" alt="image-20250201190515432" style="zoom:50%;" />

<img src="https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201190531937.png" alt="image-20250201190531937" style="zoom: 50%;" />

进行损失相加

<img src="https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250201190556201.png" alt="image-20250201190556201" style="zoom:50%;" />

## 4.RUIE: Retrieval-based Unified Information Extraction using Large Language Model--ACL2025[llm+rag]

1.动机

![image-20250525201157684](C:\Users\ZhuanZ\AppData\Roaming\Typora\typora-user-images\image-20250525201157684.png)

copyright:豆包

2.方法

x是输入y是输出，冻结大模型参数，用很多组输入输出，得到n个输出，x包括指令I+文本输入（指令I的内容如下）。得到输出之后呢，评估得到GT的概率。用这个来选指令，看看 哪个指令得到GT的概率最高。好的指令当正样本，差的指令当负样本

Task: Named Entity Recognition
Schema: [location, person, organization]
Input: The Parkinsons are a punk rock band originally from Coimbra, Portugal, formed in the
year 2000 and based in London, known for their outrageous live performances.
Output: location: Coimbra; location: Portugal; location: London.

<img src="C:\Users\ZhuanZ\AppData\Roaming\Typora\typora-user-images\image-20250529112435628.png" alt="image-20250529112435628" style="zoom: 50%;" />



a.RUIE 与传统检索方法的核心区别是什么？

**答案**：RUIE 是首个针对 UIE 的**可训练检索框架**，通过以下创新实现统一多任务处理：

- **多任务候选池**：包含 NER/RE/EE 等多任务示例，而非单一任务。
- **双重监督信号**：结合**LLM 偏好评分**（捕捉模型对示例的隐含偏好）和**关键词增强奖励模型**（对齐实体 / 关系细粒度信息），而非仅依赖语义相似度。
- **端到端训练**：通过对比学习和知识蒸馏优化双编码器检索器，支持高效推理。

b.BM25?

BM25（Best Matching 25）是一种经典的**概率检索模型**，广泛应用于信息检索系统中，用于衡量查询与文档之间的相关性。其核心思想是通过词频、逆文档频率（IDF）和文档长度归一化等因素，量化文本间的匹配程度，适用于关键词匹配、文档排序等任务。

文中初始化候选集：
在训练阶段，利用 BM25 从多任务候选池（包含 NER/RE/EE 等任务数据）中快速检索出与输入文本语义相关的初始候选示例（如 Top 100 个），降低后续 LLM 评分的计算成本

## 5.组内IPM--Retrieval augmentation for text-to-table generation

2.1 文本匹配（TeM）

- 检索机制：使用 BM25 算法从训练集检索与输入文本最相似的文本 - 表格对（排除自身）。
- 输入增强：通过模板`"example text: Xs; example table: Ts; test text: X"`拼接检索结果与原始输入，引导表头生成和内容推导。

2.2 迭代表格生成（ITG）

- 初始生成：基于 TeM 增强输入生成初步表格。
- 迭代检索：将生成表格作为查询，再次检索训练集获取更相关的表格 - 文本对，进一步增强输入（如添加表头结构`"guiding plan: Struct(Ttab)"`）。
- 终止条件：当生成表格不再变化时停止迭代，确保结构和值的准确性。

2.3 噪声感知学习（NaL）

- 噪声注入：训练时以 15% 概率随机替换真实表头为检索表中无关表头，模拟迭代过程中可能的噪声。
- 鲁棒性提升：迫使模型区分有效辅助信息，避免盲目复制错误结构，提升生成稳定性。

问题 1：Ragtable 如何解决传统方法的表头生成困难？

答案：通过  文本匹配（TeM） 检索训练集中相似文本 - 表格对，将其表头结构作为辅助输入（如模板拼接），引导模型学习潜在表头模式。例如，当输入文本缺乏 “subtitle” 等信息时，检索到的表格可提供该表头作为生成参考，避免仅依赖原始文本的局限性。

问题 2：迭代表格生成（ITG）的具体流程和优势是什么？

答案：流程为：

1. 用 TeM 生成初步表格；
2. 将初步表格作为查询，检索训练集获取更相关的表格 - 文本对；
3. 拼接新辅助信息后再次生成，重复直至结果稳定。
   优势在于多轮检索优化：通过生成表格的语义匹配，逐步逼近真实结构，尤其在表头和复杂值生成上更精准。实验显示，ITG 使检索表头与真实表头的 Jaccard 系数提升显著，低相似度（0-0.25）比例下降。

问题 3：噪声感知学习（NaL）如何提升模型鲁棒性？

答案：NaL 在训练中以 15% 概率随机替换真实表头为无关噪声（如检索表中不存在于真实表的表头），迫使模型学习区分有效信息。例如，当生成过程中混入错误表头 “country” 时，模型需通过噪声鲁棒性机制抑制该错误，优先选择真实表头 “awarding organisation”。此机制使模型在迭代中避免盲目依赖检索结果，提升对噪声输入的容错能力，实验表明 NaL 使非表头单元格 F1 提升约 2%。

## 6.百度通用抽取模型UIE--Unified Structure Generation for Universal Information Extraction fromACL2022**

![image-20250417135606973](https://github.com/NKKCCB/Rec-Home/blob/main/typora-resource/image-20250417135606973.png)

1.2.1动机

​		提出通用信息抽取统一框架

1.2.2做法

​		1.统一训练形式

​		作者本文提出了两个东西：

​	（1）SEL (结构抽取语言)  编码不同的信息抽取结构。这样所有的信息抽取任务就有了一样的输入数据结构。

SEL把所有信息抽取任务都抽象出生成SPOTNAME、ASSONAME、INFOSPAN三种。

- SPOTNAME：表示原文片段所属的实体类型

- ASSONAME：表示不同片段之间的关联关系

- INFOSPAN：表示对应spotting和associating所在原文中的信息

​	（2）SSI (结构化模式指导器) 一种基于Schema的提示机制。当输入句子时，在句子前面拼接上对应的Prompt做提示，如：[spot]人物[spot] 组织机构 [spot] 时间 [text] 1997年，史蒂夫兴奋地成为苹果公司的首席执行官。[spot]人物[spot] 组织机构 [spot] 时间 [text]都是prompt

​			2.训练

​			预训练包括三个部分

​	（1）使用 SSI+ text + SEL 训练并行对(token序列x，结构化记录y) 。 训练得到文本到结构的转化能力。

使模型能用:[spot] person [asso] work for [text]Steve became CEO of Apple in 1997., 生成:((person: Steve(work for: Apple)))。

​	（2）只使用SEL作为训练数据。结构化记录 y（None，None，y），使用前一部分来生成后一部分，并且只训练UIE的decoder部分，学习SEL语法能力。

使模型能够 ：输入((person: Steve(work for。生成:: Apple)))。

​	（3）使用text和SEL 。构造无结构的原始文本数据：（None，x'(破坏过的源文本），x''(破坏的目标spans)） 训练得到 语义编码能力（span corruption）

